{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "import string #\n",
    "import re # Regular Expression\n",
    "import pprint # Pretty Print for long texts\n",
    "from random import randint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn import tree # Decision Tree\n",
    "from sklearn import ensemble # Random Forest\n",
    "from sklearn import linear_model, preprocessing # Linear Regression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File system\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# Warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading(path):\n",
    "    inputfile = os.path.join(path)\n",
    "    with open(inputfile) as f:\n",
    "        book = f.read()\n",
    "        return book\n",
    "\n",
    "path = 'CSV_file/book/'\n",
    "bookfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "bookfiles = bookfiles[1:]\n",
    "\n",
    "books = []\n",
    "for i in bookfiles:\n",
    "    books.append(loading(path+i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf1404\\\\cocoasubrtf470\\n{\\\\fonttbl\\\\f0\\\\fmodern\\\\fcharset0 Courier;}\\n{\\\\colortbl;\\\\red255\\\\green255\\\\blue255;\\\\red0\\\\green0\\\\blue0;}\\n\\\\margl1440\\\\margr1440\\\\vieww10800\\\\viewh8400\\\\viewkind'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[0][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = re.sub(r'\\n', ' ', text) # removing the /n from the above text\n",
    "    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]', '', text) # removing teh above seen \\\\ from the text\n",
    "    text = re.sub('a0', '', text)\n",
    "    text = re.sub('\\'92t', '\\'t', text)\n",
    "    text = re.sub('\\'92s', '\\'s', text)\n",
    "    text = re.sub('\\'92m', '\\'m', text)\n",
    "    text = re.sub('\\'92ll', '\\'ll', text)\n",
    "    text = re.sub('\\'91', '', text)\n",
    "    text = re.sub('\\'92', '', text)\n",
    "    text = re.sub('\\'93', '', text)\n",
    "    text = re.sub('\\'94', '', text)\n",
    "    text = re.sub('\\.', '. ', text)\n",
    "    text = re.sub('\\!', '! ', text)\n",
    "    text = re.sub('\\?', '? ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "book_clean = []\n",
    "for i in books:\n",
    "    book_clean.append(clean_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rtf1ansiansicpg1252cocoartf1404cocoasubrtf470 fonttblf0fmodernfcharset0 Courier; colortbl;red255green255blue255;red0green0blue0; margl1440margr1440vieww10800viewh8400viewkind0 deftab720 pardpardeftab7'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_clean[0][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r': 0,\n",
       " 't': 1,\n",
       " 'f': 2,\n",
       " '1': 3,\n",
       " 'a': 4,\n",
       " 'n': 5,\n",
       " 's': 6,\n",
       " 'i': 7,\n",
       " 'c': 8,\n",
       " 'p': 9,\n",
       " 'g': 10,\n",
       " '2': 11,\n",
       " '5': 12,\n",
       " 'o': 13,\n",
       " '4': 14,\n",
       " '0': 15,\n",
       " 'u': 16,\n",
       " 'b': 17,\n",
       " '7': 18,\n",
       " ' ': 19,\n",
       " 'l': 20,\n",
       " 'm': 21,\n",
       " 'd': 22,\n",
       " 'e': 23,\n",
       " 'h': 24,\n",
       " 'C': 25,\n",
       " ';': 26,\n",
       " 'v': 27,\n",
       " 'w': 28,\n",
       " '8': 29,\n",
       " 'k': 30,\n",
       " 'x': 31,\n",
       " 'T': 32,\n",
       " 'P': 33,\n",
       " 'j': 34,\n",
       " 'G': 35,\n",
       " 'E': 36,\n",
       " 'B': 37,\n",
       " 'A': 38,\n",
       " 'K': 39,\n",
       " ',': 40,\n",
       " 'y': 41,\n",
       " 'L': 42,\n",
       " '.': 43,\n",
       " 'Y': 44,\n",
       " '-': 45,\n",
       " ':': 46,\n",
       " '/': 47,\n",
       " 'R': 48,\n",
       " 'D': 49,\n",
       " 'J': 50,\n",
       " '9': 51,\n",
       " '3': 52,\n",
       " 'W': 53,\n",
       " 'H': 54,\n",
       " 'M': 55,\n",
       " 'F': 56,\n",
       " 'X': 57,\n",
       " 'S': 58,\n",
       " 'U': 59,\n",
       " 'O': 60,\n",
       " 'I': 61,\n",
       " 'N': 62,\n",
       " 'q': 63,\n",
       " \"'\": 64,\n",
       " '\"': 65,\n",
       " '?': 66,\n",
       " '!': 67,\n",
       " 'z': 68,\n",
       " 'V': 69,\n",
       " 'Z': 70,\n",
       " '6': 71,\n",
       " 'Q': 72,\n",
       " '&': 73,\n",
       " '$': 74}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "count = 0\n",
    "for i in book_clean:\n",
    "    for char in i:\n",
    "        if char not in vocab:\n",
    "            vocab[char] = count\n",
    "            count += 1\n",
    "            \n",
    "vocab            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['<PAD>', '<EOS>', '<GO>'] # add occurance of these codes\n",
    "for code in codes:\n",
    "    vocab[code] = count\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'r',\n",
       " 1: 't',\n",
       " 2: 'f',\n",
       " 3: '1',\n",
       " 4: 'a',\n",
       " 5: 'n',\n",
       " 6: 's',\n",
       " 7: 'i',\n",
       " 8: 'c',\n",
       " 9: 'p',\n",
       " 10: 'g',\n",
       " 11: '2',\n",
       " 12: '5',\n",
       " 13: 'o',\n",
       " 14: '4',\n",
       " 15: '0',\n",
       " 16: 'u',\n",
       " 17: 'b',\n",
       " 18: '7',\n",
       " 19: ' ',\n",
       " 20: 'l',\n",
       " 21: 'm',\n",
       " 22: 'd',\n",
       " 23: 'e',\n",
       " 24: 'h',\n",
       " 25: 'C',\n",
       " 26: ';',\n",
       " 27: 'v',\n",
       " 28: 'w',\n",
       " 29: '8',\n",
       " 30: 'k',\n",
       " 31: 'x',\n",
       " 32: 'T',\n",
       " 33: 'P',\n",
       " 34: 'j',\n",
       " 35: 'G',\n",
       " 36: 'E',\n",
       " 37: 'B',\n",
       " 38: 'A',\n",
       " 39: 'K',\n",
       " 40: ',',\n",
       " 41: 'y',\n",
       " 42: 'L',\n",
       " 43: '.',\n",
       " 44: 'Y',\n",
       " 45: '-',\n",
       " 46: ':',\n",
       " 47: '/',\n",
       " 48: 'R',\n",
       " 49: 'D',\n",
       " 50: 'J',\n",
       " 51: '9',\n",
       " 52: '3',\n",
       " 53: 'W',\n",
       " 54: 'H',\n",
       " 55: 'M',\n",
       " 56: 'F',\n",
       " 57: 'X',\n",
       " 58: 'S',\n",
       " 59: 'U',\n",
       " 60: 'O',\n",
       " 61: 'I',\n",
       " 62: 'N',\n",
       " 63: 'q',\n",
       " 64: \"'\",\n",
       " 65: '\"',\n",
       " 66: '?',\n",
       " 67: '!',\n",
       " 68: 'z',\n",
       " 69: 'V',\n",
       " 70: 'Z',\n",
       " 71: '6',\n",
       " 72: 'Q',\n",
       " 73: '&',\n",
       " 74: '$',\n",
       " 75: '<PAD>',\n",
       " 76: '<EOS>',\n",
       " 77: '<GO>'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reversing the dictionary as\n",
    "int_vo = {}\n",
    "for i, j in vocab.items():\n",
    "    int_vo[j] = i\n",
    "    \n",
    "int_vo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rtf1ansiansicpg1252cocoartf1404cocoasubrtf470 fonttblf0fmodernfcharset0 Courier; colortbl;red255green255blue255;red0green0blue0; margl1440margr1440vieww10800viewh8400viewkind0 deftab720 pardpardeftab720sl280partightenfactor0 f0fs24 cf2 expnd0expndtw0kerning0 outl0strokewidth0 strokec2 The Project Gutenberg EBook of Anna Karenina, by Leo Tolstoy This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for i in book_clean:\n",
    "    for sen in i.split('.'):\n",
    "        sentences.append(sen + '.')\n",
    "\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Sentence to Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer numbers are from vocab\n",
    "final_sentence = []\n",
    "for i in sentences:\n",
    "    b = []\n",
    "    for char in i:\n",
    "        b.append(vocab[char])\n",
    "    final_sentence.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 4, 5]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as in this assinging the values to the character in sentences\n",
    "final_sentence[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99215\n",
      "33072\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(final_sentence, test_size=0.25, random_state=2)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8907\n"
     ]
    }
   ],
   "source": [
    "maxt = max([len(sentence) for sentence in train])\n",
    "print(maxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the sentence between length of 10 to 300 as to make training faster\n",
    "train_sort = []\n",
    "min_length = 10\n",
    "max_length = 300\n",
    "for i in range(min_length, max_length+2):\n",
    "    for j in train:\n",
    "        if(len(j) == i):\n",
    "            train_sort.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n"
     ]
    }
   ],
   "source": [
    "maxt = max([len(sentence) for sentence in train_sort])\n",
    "print(maxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we generate noise in sentence (word by word) to make some random changes into training dataset\n",
    "# so model can justify the changes and work accordingly\n",
    "letter = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "def noise(sentence, threshold):\n",
    "    noisy = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        rand = np.random.uniform(0, 0.9, 1)\n",
    "        if rand < threshold:\n",
    "            noisy.append(sentence[i])\n",
    "        else:\n",
    "            new_rand = np.random.uniform(0, 0.9, 1)\n",
    "            if new_rand > 0.67:\n",
    "                if i == (len(sentence) - 1):\n",
    "                    continue\n",
    "                else:\n",
    "                    i += 1\n",
    "            elif new_rand < 0.33:\n",
    "                random_letter = np.random.choice(letter, 1)[0]\n",
    "                noisy.insert(vocab[random_letter])\n",
    "                noisy.insert(sentence[i])\n",
    "            else:\n",
    "                pass\n",
    "        i += 1\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create noise sentence by sentence\n",
    "\n",
    "letter = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "def nois(sentence, threshold):\n",
    "    noisy = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        rand = np.random.uniform(0, 0.9, 1)\n",
    "        if rand < threshold:\n",
    "            noisy.append(sentence[i])\n",
    "        else:\n",
    "            random_letter = np.random.choice(letter, 1)[0]\n",
    "            a = vocab[random_letter]\n",
    "            c = len(sentence)\n",
    "            random_index = randint(0, c)\n",
    "            noisy.insert(random_index, a)\n",
    "        i += 1\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 33, 23, 2, 7, 13, 1, 1, 41, 43]\n",
      "[19, 33, 23, 10, 10, 13, 1, 1, 41, 43]\n"
     ]
    }
   ],
   "source": [
    "f = nois(train_sort[0], 0.8) # sample generate noise in first sentence with threshold of 0.8\n",
    "print(f)\n",
    "print(train_sort[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    }
   ],
   "source": [
    "ef = int_vo[5]\n",
    "print(ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "87923\n"
     ]
    }
   ],
   "source": [
    "for i in train_sort[0:10]: # length of first 10 sentence of train_sort\n",
    "    print(len(i))\n",
    "print(len(train_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_train = [] #noised train sorted data\n",
    "b = 0.9 # threshold\n",
    "for sentence in train_sort:\n",
    "    f = nois(sentence, b)\n",
    "    noisy_train.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in noisy_train[:10]:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n",
      "[19, 33, 23, 10, 10, 13, 1, 1, 41, 43]\n",
      "[19, 33, 23, 10, 10, 13, 1, 1, 41, 43]\n"
     ]
    }
   ],
   "source": [
    "maxt = max([len(sentence) for sentence in noisy_train])\n",
    "print(maxt)\n",
    "print(noisy_train[0])\n",
    "print(train_sort[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence(batch):\n",
    "    '''Pad sentences with <PAD> so that each sentence has the same length for both train_sort & noisy_train'''\n",
    "    max_sentence = max([len(sentence) for sentence in batch])\n",
    "    return [sentence + [vocab['<PAD>']] * (max_sentence - len(sentence)) for sentence in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_train = np.array(pad_sentence(train_sort))\n",
    "pad_noisy_train = np.array(pad_sentence(noisy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301,)\n",
      "(301,)\n"
     ]
    }
   ],
   "source": [
    "print(pad_train[2].shape)\n",
    "print(pad_noisy_train[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 40)          6720      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 20)          4880      \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, None, 1)          21        \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,621\n",
      "Trainable params: 11,621\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(40, return_sequences=True, input_shape=(None, 1)))\n",
    "model.add(LSTM(20, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc']) #loss functionality\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the pad_train & pad_noisy_train\n",
    "pd_train = preprocessing.normalize(pad_train)\n",
    "pd_noisy_train = preprocessing.normalize(pad_noisy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01482393]\n",
      " [0.02886765]\n",
      " [0.        ]\n",
      " [0.01014269]\n",
      " [0.02184579]\n",
      " [0.00390103]\n",
      " [0.01560413]\n",
      " [0.01014269]\n",
      " [0.02184579]\n",
      " [0.03354889]]\n"
     ]
    }
   ],
   "source": [
    "y = pd_train.shape\n",
    "a = pd_train.reshape(y[0], y[1], 1)\n",
    "print(a[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01482393]\n",
      " [0.02886765]\n",
      " [0.        ]\n",
      " [0.01014269]\n",
      " [0.02184579]\n",
      " [0.00390103]\n",
      " [0.01560413]\n",
      " [0.01014269]\n",
      " [0.02184579]\n",
      " [0.03354889]]\n"
     ]
    }
   ],
   "source": [
    "# similarly with noise data\n",
    "z = pd_noisy_train.shape\n",
    "b = pd_noisy_train.reshape(z[0], z[1], 1)\n",
    "print(b[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(a, b, batch_size=10, epochs=40, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same procedure for Testing data\n",
    "test_sort = []\n",
    "min_length = 10\n",
    "max_length = 300\n",
    "for i in range(min_length, max_length+2):\n",
    "    for j in train:\n",
    "        if (len(j) == i):\n",
    "            test_sort.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_test = [] #noised test_sorted data\n",
    "b = 0.9 # threshold\n",
    "for sentence in train_sort:\n",
    "    f = nois(sentence, b)\n",
    "    noisy_test.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_test = np.array(pad_sentence(test_sort))\n",
    "pad_noisy_test = np.array(pad_sentence(noisy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the pad_train & pad_noisy_train\n",
    "pd_test = preprocessing.normalize(pad_test)\n",
    "pd_noisy_test = preprocessing.normalize(pad_noisy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd_test.shape\n",
    "a1 = pd_test.reshape(y[0], y[1], 1)\n",
    "print(a1[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly with noise data\n",
    "z = pd_noisy_test.shape\n",
    "b1 = pd_noisy_test.reshape(z[0], z[1], 1)\n",
    "print(b1[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(a1, b1, verbose=0)\n",
    "print('Loss: %f, Accuracy: %f' %(loss, acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    yhat = model.predict_classes(X, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5024435cbfb8faf8bbbcdcac495405647c340211599983c8ce8e312773b27794"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
